{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS207 Systems Development for Computational Science\n",
    "## Final Project\n",
    "## Milestone 1 \n",
    "\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2019**<br/>\n",
    "**Instructor**: David Sondak <br/>\n",
    "**Team #14**: Fantastic Four<br/>\n",
    "**Students**: Daniel Cox, Anna Davydova, Stephen Moon, Valentina Toll Villagra   <br/>\n",
    "**Git Repository**:https://github.com/IACS-CS-207-FantasticFour/cs207-FinalProject\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*“Nothing takes place in the world whose meaning is not that of some maximum or minimum.”*\n",
    "― Leonhard Euler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTRODUCTION:**<br/>\n",
    "<br/>\n",
    "This project aims to deliver an Automatic Differentiation Software DeltaPi that will calculate derivatives efficiently and accurately, making it useful across a wide spectrum of applications. In this package, we will execute the forward mode of Automatic Differentiation and provide an extension for pricing of financial options. We are especially excited about the applications of this software on Wall Street. <br/>\n",
    "<br/>\n",
    "Derivatives are powerful and ubiquitous. Their use spans from gradients and Hessians in machine learning applications to hedge sensitivities in financial markets. However, of the four existing methods for derivative calculation only Automatic  Differentiation (AD) combines interpretability, efficiency and accuracy. Manual differentiation is inefficient and susceptible to error.(Baydin et. al., 2018). Numerical differentiation, while easy and fairly quick in its implementation, is also inaccurate, prone to rounding and truncation errors (Jerrell, 1997).  It does not scale well, which makes it a poor choice for machine learning models. Symbolic differentiation, while accurate, can become incredibly complex facing \"expression swell\" issues (Corliss, 1988). AD overcomes these issues, via its application of the chain rule and a step by step approach to differentiation, and accurately computes derivatives with asymptotic efficiency. (Baydin et.al, 2018). <br/>\n",
    "<br/>\n",
    "Our software DeltaPi will implement AD methods, allowing the end user to benefit from its accuracy and efficiency.  Our goal is to produce a package that can handle a wide variety of uses beyond simple scalar functions with an additional extension for option trading applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BACKGROUND:** <br/>\n",
    "<br/>\n",
    "Generally speaking, in any computer program a function can be broken down into its elementary function components (unary or binary) such as addition, subtraction  log, sin, sqrt etc (Heath, 2018).  Since the value of partial derivatives of these elementary functions can be easily calculated, then the value of the entire function can be calculated via the application of the chain rule.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us walk through this process in more detail. Fundamentally, Chain Rule is the key pillar to the AD process. Recall that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\dfrac{\\partial h}{\\partial t} = \\dfrac{\\partial h}{\\partial u}\\dfrac{\\partial u}{\\partial t}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the definition of the Chain Rule to a function that contains to functions as follows: h(u(t),v(t)) (Sondak, 2019).:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "  \\displaystyle \n",
    "  \\frac{\\partial h}{\\partial t} = \\frac{\\partial h}{\\partial u}\\frac{\\partial u}{\\partial t} + \\frac{\\partial h}{\\partial v}\\frac{\\partial v}{\\partial t}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result, from a perspetive of a gradient, leads us to this general rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "  \\nabla_{x}h = \\sum_{i=1}^{n}{\\frac{\\partial h}{\\partial y_{i}}\\nabla y_{i}\\left(x\\right)}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, in a nutshell, the AD process consist of breaking down the function into its elementary components and carrying out the differentiation process in sequential order of these operations while multiplying through with the chain rule. The output of this process is a dual number containing the value of the function along with its derivative.  We can visualize this process with a computational graph and a table that contains the trace of the calculations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for a simple function (*adapted from class exercises*): $$f\\left(x,y\\right) = (sin(x)-cos(y))^2.$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward mode AD graph looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"simple_example.png\" width=\"500\" height=\"240\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding computational table for values $x=\\frac{\\pi}{2}$ and $y=\\frac{\\pi}{3}$looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Trace    | Elementary Operation &nbsp;&nbsp;&nbsp;| Current Function Value &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     | Derivative  &nbsp;&nbsp;&nbsp;&nbsp;   | Partial Derivative w.r.t. x                            | Partial Derivative w.r.t. y                       |evaluate f(x) and its derivative: (f(x), f'(x))                   |\n",
    "| :------: | :-------------------------------------:| :--------------------------------------------------------:|:-------------------------------------: | :-------------------------------------------------:|:----------------------------------------------:|:---------------------------------------:\n",
    "| $x_{1}$  | $x_{1}$                                |          $\\frac{\\pi}{2}$                                              |$\\dot {x_1}$                                     |      $1$                                           |      $0$                                       |$\\left(\\frac{\\pi}{2}, 1\\right)$                                                                  |\n",
    "| $x_{2}$  | $x_{2}$                                |          $\\frac{\\pi}{3}$                                              |$\\dot {x_2}$                                     |      $0$                                           |      $1$                                       |$\\left(\\frac{\\pi}{3},1 \\right)$                                                                  |\n",
    "| $x_{3}$  | $sin(x_1)$                             |          $1$                                          |$cos(x_1)\\dot{x_1}$                          |      $0$                                          |      $0$                                       |$\\left(1,0)\\right)$                                                        |\n",
    "| $x_{4}$  | $cos(x_2)$                         |               $\\frac{1}{2}$                                            |$-sin(x_2)\\dot{x_2}$                     |      $0$                                          |    $-\\frac{\\sqrt{3}}{2}$                             |$\\left(\\frac{1}{2},-\\frac{\\sqrt{3}}{2}\\right )$                                                         |\n",
    "| $x_{5}$  | $x_3-x_4$                          |          $\\frac{1}{2}$                                        |$\\dot{x_3}-\\dot{x_5}$           |     $0$                                        |   $\\frac{\\sqrt{3}}{2}$                            |$\\left(\\frac{1}{2},\\frac{\\sqrt{3}}{2}\\right)$                                                         |\n",
    "| $x_{6}$  | $x_5^2$                             |    $\\frac{1}{4}$                                     |  $2x_5\\dot{x_5}$                         |                        $0$                            |       $\\frac{\\sqrt{3}}{2}$ |           $\\left(\\frac{1}{4},\\frac{\\sqrt{3}}{2}\\right)$      |                                                                                                        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note not only the simplicity of differentiation process on elementary functions carried along by the chain rule but relatively interpretable process that could be useful to our end-users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HOW TO USE OUR PACKAGE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Pre-requisites:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "​ Be x,y,z,... input variables\n",
    "​ Be f(x,y,z,...) the output of function f at (x,y,z,...)\n",
    "​ Be AutoDiff the class \n",
    "\n",
    "1) Create an instance of the AutoDiff class\n",
    "x = AutoDiff(val, derv)\n",
    "\n",
    "where:\n",
    "​ val = the value of x at the desired point\n",
    "​ derv = the initial value of the derivative of x\n",
    "    ​ 1 for calculating the partial derivative df/dx\n",
    "    ​ 0 for calculating the partial derivative df/dy,df/dz,...\n",
    "\n",
    "2) Repeat the process of creating an instance of the AutoDiff class for each input variable\n",
    "y = AutoDiff(val, derv)\n",
    "z = AutoDiff(val, derv)\n",
    "...\n",
    "\n",
    "where:\n",
    "​ val = the value of the instance variable (y,z,...) at the desired point\n",
    "​ derv = the initial value of the derivative of the instance variable (y,z,...)\n",
    "    ​ 1 for calculating the partial derivative of the instance variable (df/dy for y, df/dz for z,...)\n",
    "    ​ 0 for calculating the partial derivative of another variable that is not the instance one (df/dx,df/dz for y, df/dx,df/dy for z,...)\n",
    "    \n",
    "\n",
    "3) Enter the function of interest\n",
    "f = function of x,y,z,...\n",
    "\n",
    "where function can contain the following operations:\n",
    "​ +\n",
    "​ -\n",
    "​ * \n",
    "​ /\n",
    "​ **\n",
    "​ exp()\n",
    "​ sqrt()\n",
    "​ sin()\n",
    "​ cos()\n",
    "​ tan()\n",
    "\n",
    "4) Print f.val to get the calculated value of f at the specified point (x,y,z,...)\n",
    "print(f.val)\n",
    "\n",
    "5) Print f.derv to get the calculated value of df/dx or df/dy or df/dz or ... at the specified point (x,y,z,...)\n",
    "print(f.derv)\n",
    "\n",
    "\n",
    "#### Example\n",
    "\n",
    "​ For values:\n",
    "x = 3\n",
    "y = 4\n",
    "f = x + 2 * y\n",
    "\n",
    "​ Be the goal to calculate the partial derivatives df/dx at (x,y)=(3,4)\n",
    "\n",
    "1) Create an instance of the AutoDiff class\n",
    "x = AutoDiff(3, 1)\n",
    "\n",
    "2) Repeat the process of creating an instance of the AutoDiff class for y\n",
    "y = AutoDiff(4, 0)\n",
    "\n",
    "3) Enter the function of interest\n",
    "f = x + 2 * y\n",
    "\n",
    "4) Print f.val to get the calculated value of f at (3,4)\n",
    "print(f.val)\n",
    "\n",
    "5) Print f.derv to get the calculated value of df/dx at (3,4)\n",
    "print(f.derv)\n",
    "\n",
    "\n",
    "​ Be the goal to calculate the partial derivatives df/dy at the same (x,y)=(3,4)\n",
    "\n",
    "1) Create an instance of the AutoDiff class\n",
    "x = AutoDiff(3, 0)\n",
    "\n",
    "2) Repeat the process of creating an instance of the AutoDiff class for y\n",
    "y = AutoDiff(4, 1)\n",
    "\n",
    "3) Enter the function of interest\n",
    "f = x + 2 * y\n",
    "\n",
    "4) Print f.val to get the calculated value of f at (3,4) - will be the same from df/dx\n",
    "print(f.val)\n",
    "\n",
    "5) Print f.derv to get the calculated value of df/dy at (3,4) print(f.derv)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "#### To use all_derivatives() function\n",
    "\n",
    "​ Be x,y,z,... input variables\n",
    "​ Be func(x,y,z,...) the output of function f at (x,y,z,...)\n",
    "​ Be all_derivatives the function that uses AutoDiff class \n",
    "\n",
    "1) Create a vector containing the input variables\n",
    "in_vars = [x,y,z,...]\n",
    "\n",
    "where:\n",
    "​ x,y,z,... are variable names \n",
    "\n",
    "2) Create a vector containing the variable values\n",
    "in_vals = [val_x,val_y,val_z,...]\n",
    "\n",
    "where:\n",
    "​ val_x = the value of x at which to evaluate the function\n",
    "​ val_y = the value of y at which to evaluate the function\n",
    "​ val_z = the value of z at which to evaluate the function\n",
    "...\n",
    "\n",
    "3) Enter the function to be evaluated\n",
    "func = function of x,y,z,...\n",
    "\n",
    "where function can contain the following operations:\n",
    "​ +\n",
    "​ -\n",
    "​ * \n",
    "​ /\n",
    "​ **\n",
    "​ exp()\n",
    "​ sqrt()\n",
    "​ sin()\n",
    "​ cos()\n",
    "​ tan()\n",
    "\n",
    "4) Run all_derivatives()\n",
    "result = allderivatives(func, in_vars, in_vals)\n",
    "\n",
    "5) Print result.val to get the calculated value of func at (in_vars)\n",
    "print(result.val)\n",
    "\n",
    "6) Print result.derv_vals to get vector of calculated partial derivative values, one for each variable in in_vars print(result.derv_vals)\n",
    "\n",
    "#### Example\n",
    "\n",
    "​ For values:\n",
    "x = 3\n",
    "y = 4\n",
    "z = 5\n",
    "func = x + 2 * y - z\n",
    "\n",
    "\n",
    "1) Create a vector containing the input variables\n",
    "in_vars = [x,y,z]\n",
    "\n",
    "2) Create a vector containing the variable values\n",
    "in_vals = [3,4,5]\n",
    "\n",
    "3) Enter the function to be evaluated\n",
    "func =  x + 2 * y - z\n",
    "\n",
    "4) Run all_derivatives()\n",
    "result = all_derivatives(func, in_vars, in_vals)\n",
    "\n",
    "5) Print result.val to get the calculated value of func at (in_vars)\n",
    "print(result.val)\n",
    "\n",
    "6) Print result.derv_vals to get vector of calculated partial derivative values, one for each variable in in_vars print(result.derv_vals)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "#### To use multi_func_all_derivatives() function\n",
    "\n",
    "​ Be x,y,z,... input variables\n",
    "​ Be func(x,y,z,...) the output of function f at (x,y,z,...)\n",
    "​ Be multi_func_all_derivatives the function that uses AutoDiff class \n",
    "\n",
    "1) Create a vector containing the input variables\n",
    "in_vars = [x,y,z,...]\n",
    "\n",
    "where:\n",
    "​ x,y,z,... are variable names \n",
    "\n",
    "2) Create a vector containing the variable values\n",
    "in_vals = [val_x,val_y,val_z,...]\n",
    "\n",
    "where:\n",
    "​ val_x = the value of x at which to evaluate the function\n",
    "​ val_y = the value of y at which to evaluate the function\n",
    "​ val_z = the value of z at which to evaluate the function\n",
    "...\n",
    "\n",
    "3) Create a vector containing the functions to be evaluated\n",
    "funcs = [func_1,func_2,func_3,...]\n",
    "\n",
    "where:\n",
    "func_n = function of x,y,z,...\n",
    "\n",
    "where func_n can contain the following operations:\n",
    "​ +\n",
    "​ -\n",
    "​ * \n",
    "​ /\n",
    "​ **\n",
    "​ exp()\n",
    "​ sqrt()\n",
    "​ sin()\n",
    "​ cos()\n",
    "​ tan()\n",
    "\n",
    "4) Run multi_func_all_derivatives()\n",
    "result = multi_func_all_derivatives(funcs, in_vars, in_vals)\n",
    "\n",
    "5) Print result.out_vals to get vector of calculated values of all funcs each at (in_vars)\n",
    "print(result.out_vals)\n",
    "\n",
    "6) Print result.out_derv_matrix to get the Hessian matrix of partial derivatives evaluated at (in_vars)\n",
    "print(result.out_derv_matrix)\n",
    "\n",
    "#### Example\n",
    "​ For values:\n",
    "x = 3\n",
    "y = 4\n",
    "z = 5\n",
    "func_1 = x + 2 * y - z\n",
    "func_2 = x - exp(y) + z\n",
    "\n",
    "1) Create a vector containing the input variables\n",
    "in_vars = [x,y,z]\n",
    "\n",
    "\n",
    "2) Create a vector containing the variable values\n",
    "in_vals = [3,4,5]\n",
    "\n",
    "3) Create a vector containing the functions to be evaluated\n",
    "func_1 = x + 2 * y - z\n",
    "func_2 = x - exp(y) + z\n",
    "funcs = [func_1,func_2]\n",
    "\n",
    "4) Run multi_func_all_derivatives()\n",
    "result = multi_func_all_derivatives(funcs, in_vars, in_vals)\n",
    "\n",
    "5) Print result.out_vals to get vector of calculated values of all funcs each at (in_vars)\n",
    "print(result.out_vals)\n",
    "\n",
    "6) Print result.out_derv_matrix to get the Hessian matrix of partial derivatives evaluated at (in_vars) print(result.out_derv_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOFTWARE ORGANIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss how you plan on organizing your software package.<br/>\n",
    "\n",
    "1.  What will the directory structure look like?\n",
    "2.  What modules do you plan on including? What is their basic functionality?\n",
    "3.  Where will your test suite live? Will you use TravisCI? CodeCov?\n",
    "4.  How will you distribute your package (e.g. PyPI)?\n",
    "5.  How will you package your software? Will you use a framework? If so, which one and why? If not, why not?\n",
    "Other considerations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPEMENTATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Our software will provide the user with a package he or she can use to evaluate functions of multiple variables at a specified point while at the same time evaluating the partial derivatives of the function at that same point. A class will be provided to perform simple calculations, and two functions will be provided to handle vector inputs and outputs.\n",
    "\n",
    "Our strategy will be to create input-variable objects in python that once defined may be used in python calculations as if they were standard python variables of type float. However, when calculations are done with these Autdiff objects, a partial derivative with respect to one of the input variables will be calculated and carried along. The object representing the result, then, will contain an attribute that contains the value of the function at the specified point (val) and an attribute that contains a partial derivative with respect to one of the input variables (derv). Which partial derivative to calculate will be specified by the user by intiating the derv attribute of one input variable to 1 and the derv attributes of the other input variables to 0.  The calculation can then be repeated to get partial derivatives with respect to other input variables by changing which input variable’s derv attribute is intiated to 1.\n",
    "\n",
    "*The AutDiff() class*\n",
    "\n",
    "Most of the functionality of our package will be implemented in a single class called AutoDiff( ). Below is a skeleton of that class. As mentioned above, it will have two attributes self.val and self.derv that respectively will hold the value of the function being evaluated and the partial derivative of the function being evaluated up to some point in the calculation.\n",
    "\n",
    "Autodiff() will have many methods. Each will be short and designed to replace a standard mathematical operation. Thus, for example there will be a method \\_\\_add\\_\\_ that will overload the standard addition operator (+) and thus specify how objects of type AutoDiff will be added to scalar values or to each other. And similar methods will be included to overload the other standard mathematical operators (- * / ^). Also, methods will be included to replace the \n",
    "numpy functions (exp, ln, log, sin, cos, tan, sqrt) so that these functions can be used as well in any calculations involving AutoDiff objects.\n",
    "\n",
    "In the skeleton below, two of AutoDiff’s methods, multiplication and exponentiation, have been fully coded to illustrate how the desired functionality will be implemented generally. Central to all methods is the principle that each method will return a new AutDiff object that will represent the next stage in the function’s evaluation. For example, if x is an AutDiff object, and it is multiplied by the scalar 2 (x\\*2), then x’s \\_\\_mul\\_\\_ method will be called and it will return a new object that will represent x\\*2. The AutoDiff.\\_\\_mul\\_\\_ method will perform this calculation as follows. It will assume that 2 is another object of type AutoDiff, and it will try to multiply the two AutoDiff objects together. It will find that 2 is not an AutoDiff object, fall to its except block, and then make two calculations: 2 * x.val and 2 * x.derv and store these values in the returned object’s val and derv attributes. If the 2 had been to the right of x (2*x), then the \\_\\_rmul\\_\\_ method would have been called, and it would have executed the same operation.  \n",
    "\n",
    "More subtle is the case where two AutoDiff objects, say x and y, are multiplied together x\\*y. Here x’s \\_\\_mul\\_\\_ method will be called. It will find that the two objects are instances of AutoDiff, and it will 1) multiply x.val and y.val together and store the result as the returned object’s val attribute, and 2) calculate (x.derv\\*y.val + y.derv\\*x.val) and store this value as the returned object’s derv attribute. Notice in the second calculation the derivatives from both objects x and y are carried along as a sum. Recall, however, that the derv value of either x or y will have been initiated to 0, so one term in the sum (x.derv\\*y.val + y.derv\\*x.val) will always evaluate to 0. In this way, the algorithm can automatically choose the proper derivative to carry along for subsequent calculations (here either that of x or that of y) when two Autodiff objects collide. This same approach can be used more generally and will be used when overloading the division and power operators.\n",
    "\n",
    "Also fully coded in the AutoDiff skeleton below is the exponential function. This works as follows. For a given AutoDiff object, say x, when exp(x) is executed, x’s exp() function will be called on itself. It will use numpy’s exp() function to evaluate np.exp(x.val) and store this value in the return object’s val attribute. It will evaluate the derivative of exp(x.value) —which in this case is just exp(x.value)— and store this value in the return object’s derv attribute. Our preliminary tests indicate that this will work for all the numpy fuctions: np.exp(), np.ln(), np.log(), np.sqrt(), np.sin(), np.cos(), np.tan(), so this is how they will be implemented. \n",
    "\n",
    "Autdiff objects will be fully functional replacements for python float variables such that once the AutoDiff class is defined, these objects can be used to perform forward autodifferentiation on any multi-input, single output function at any specified point. \n",
    "\n",
    "*Skeleton of AutoDiff()*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class AutoDiff():\n",
    "    '''Class to do automatic differentiation'''\n",
    "\n",
    "    def __init__(self, val, derv=1):\n",
    "        self.val = float(val)\n",
    "        self.derv = float(derv)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        try:\n",
    "            return AutoDiff(self.val * other.val, self.derv * other.val + \n",
    "                                                          other.derv* self.val) \n",
    "        except:\n",
    "            return AutoDiff(self.val * other, self.derv * other)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return AutoDiff(self.val * other, self.derv * other)\n",
    "\n",
    "    def __div__(self, other):\n",
    "\n",
    "    def __rdiv__(self, other):\n",
    "    \n",
    "    def __add__(self, other):\n",
    "\n",
    "    def __radd__(self, other):\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "\t\n",
    "    def __pow__(self, other):\n",
    "\n",
    "    def __rpow__(self, other):\n",
    "\n",
    "    def exp(self):\n",
    "\t\treturn AutoDiff(np.exp(self.val), np.exp(self.val)*self.derv)\n",
    "\n",
    "\tdef sin(self):\n",
    "\n",
    "    def cos(self):\n",
    "    \n",
    "    def tan(self):\n",
    "    \n",
    "    def sqrt(self):\n",
    "\t\n",
    "    def ln(self):\n",
    "\t\n",
    "    def log(self):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The all_derivatives() function*\n",
    "\n",
    "This function will allow the user to calculate all partial derivatives at once without having to manually initiate the derv attributes of the input variables several times and each time repeat the calculation.\n",
    "\n",
    "Psuedo code for this function is listed below. It takes advantage of the AutoDiff() class.\n",
    "\n",
    "\n",
    "    def all_derivatives(func, in_vars, in_vals):\n",
    "        ***\n",
    "        Inputs:\n",
    "\n",
    "            func():   —the function to be evaluated\n",
    "            in_vars:  —a vector containing the input variables.\n",
    "            in_vals:  —a vector of values, one for each variable in in_vars that specify  \n",
    "                         a point where the function and its partial derivatives are to be evaluated.\n",
    "        Returns:\n",
    "\n",
    "            func():    —the function evaluated\n",
    "            val:       —the value of func( ) at the input point given by in_vals\n",
    "            derv_vals: —a vector of partial derivative values, one for each input variable in in_vars.         \t        \n",
    "        ***\n",
    "\n",
    "        Steps:\n",
    "        * make a vector diff_vars which  coorepsonds to in_vars but all\n",
    "          values are AutoDiff objects.\n",
    "        * Set the derv value of the first element of diff_vars to 1 and the derv\n",
    "          values of the rest of the diff_vars elements to 0.\n",
    "\n",
    "        * Create the derv_vals vector that will hold the calculated partial derivatives\n",
    "\n",
    "        * For i in range( the number of elements in in_vars):\n",
    "            * Set the derv value of each element of in_vars to 0\n",
    "            * Set  the derv value of the ith element to 1.\n",
    "            * Evaluate func()  \n",
    "\n",
    "            * If it is the first time thought the loop:\n",
    "                  set val to the val of the AutoDiff output object\n",
    "            * Else:\n",
    "                * Store derivative of the output AutoDiff object as the ith element of derv_vals\n",
    "\n",
    "        * Return  func, val, derv_vals\n",
    "\n",
    "\n",
    "*The multi_func_all_derivatives() function*\n",
    "\n",
    "This function will handle vector inputs and outputs. It will allow the user to specify: a vector of functions, a vector of input variables, and a vectors of values defining a specific point. It will return: the input vector containing the functions, a vector of output values (one for each function evaluated at the specified point), and a Hessian matrix of partial derivatives containing partial derivatives of all functions evaluated at the specified point. Consider the equation below as an example:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix} f(x,y) \\\\ g(x,y)\\end{pmatrix} =\n",
    "\\begin{pmatrix} 2xy \\\\ x+y^2 \\end{pmatrix} at \\begin{pmatrix} x=a \\\\ y=b \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Here the multi_func_all_derivatives() function would return:  \n",
    "1) funcs, a vector of the functions that were evaluated ($2xy$,  $x+y^2$)  \n",
    "2) out_vals, a vector of the values of each function after evaluation $(f(a,b), g(a,b))$  \n",
    "3) out_derv_matrix, a matrix containing the partial derivatives evaluated at the point (a, b)\n",
    "\n",
    "Pseudo code for this function is below. It takes advantge of the AutoDiff() class and the all_derivatives() function:\n",
    "\n",
    "    def multi_func_all_derivatives(funcs, in_vars, in_vals):\n",
    "        ***\n",
    "        Inputs: \n",
    "            funcs:    —a vector of input functions\n",
    "            in_vars: —a vector of input variables\n",
    "            in_vals:  —a vector of input values specifying a point.\n",
    "\n",
    "        Returns:\n",
    "            funcs: \t   —the vector of input functions\n",
    "            out_vals —a vector of output values, one for each function\n",
    "            out_derv_matrix —the Hessian matrix of partial derivatives evaluated at the input point\t\t\t\t\t \n",
    "        ***\n",
    "        Steps:\n",
    "        •\tCreate an empty vector out_vals\n",
    "        •\tCreate an empty matrix out_derv_matrix\n",
    "        •\tFor each function in funcs:\n",
    "                - Run all_derivatives(func, in_vars, in_vals) \n",
    "                - Store the returned val in the vector out_vals\n",
    "                - Store the returned vector derv_vals in the matrix out_derv_matrix\n",
    "\n",
    "        •\tReturn funcs, out_vals, out_derv_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTENSION**$\\text{**}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to extend our DeltaPi package into a second package that would be useful to options traders as they estimate first and second order derivatives of the Black Scholes options pricing equation. Our goal is to provide a more efficient and accurate process for these calculations. As an example of this application let us consider an American option that gives its holder the right but not the obligation to buy (call) or sell (put) the underlying financial asset S at a predetermined price (strike) K during a predetermined time period  until date T.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The payoff function for this option with stock price $S_t$ at a time t and strike K is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "  V(S_t,K) =\n",
    "    \\begin{cases}\n",
    "      max(K-S_t,0) & \\text{(put)}\\\\\n",
    "      max(S_t-K,0) & \\text{(call)}\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Black-Scholes Equation describes the option price overtime as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial V}{\\partial t}+\\frac{1}{2}\\sigma^2 S^2\\frac{\\partial^2 V}{\\partial S^2}+r S\\frac{\\partial V}{\\partial S} -r V = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensitivities (known as Greeks among the options traders) of an option with price V are calculated are first and second order derivatives as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\varDelta=\\frac{\\partial V}{\\partial S_o}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\varGamma = \\frac{\\partial^2 V}{\\partial S^2_o}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tau=\\frac {\\partial V}{\\partial \\sigma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\varTheta=\\frac{\\partial C}{\\partial t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Rho}=\\frac{\\partial C}{\\partial r}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Vanna}=\\frac{\\partial^2 V}{\\partial S_o\\sigma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of a call option can be expressed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C(S_{t},t)=N(d_{1})S_{t}-N(d_{2})PV(K)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of a put option can be expressed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}P(S_{t},t)=Ke^{-r(T-t)}-S_{t}+C(S_{t},t)\\\\=N(-d_{2})Ke^{-r(T-t)}-N(-d_{1})S_{t}\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:<br/>\n",
    "   $d_{1}={\\frac {1}{\\sigma {\\sqrt {T-t}}}}\\left[\\ln \\left({\\frac {S_{t}}{K}}\\right)+\\left(r+{\\frac {\\sigma ^{2}}{2}}\\right)(T-t)\\right]\\\\d_{2}=d_{1}-\\sigma {\\sqrt {T-t}}\\\\PV(K)=Ke^{-r(T-t)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N(x)=\\frac{1}{\\sqrt {2\\pi}}\\int^x_{-\\inf}e^{\\frac{-z^2}{2}}dz$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to apply our automatic differentiation to compute the first and second order derivatives and will compare computational time (efficiency) and accuracy of our AD process vs. symbolic and numerical differentiation solutions for Black Scholes Greeks.  This will require forward and reverse mode AD (Homescu, 2011)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{**}$*We understand that extension was not part of the formal assignment for this Milestone but we were hoping to get some early feedback if possible as we are very excited to start working on this idea. Thank you!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REFERENCES:**<br/>\n",
    "Atilim Baydin. Automatic Differentiation in Machine Learning: a Survey. Journal of Machine Learning Research, 2018. <br/> \n",
    "George F. Corliss. Application of differentiation arithmetic, volume 19 of Perspectives in Computing, pages 127–48. Academic Press, Boston, 1988.<br>\n",
    "Michael Heath. Scientific Computing: An Introductory Survey. Society for Industrial and Applied Mathematics. 8(6):367, 2018.<br/> \n",
    "Christina Homescu. Adjoints and automatic (algorithmic) differentiation in computational finance. arXiv:1107.1831v1 10 Jul 2011.<br/>\n",
    "Max E. Jerrell. Automatic differentiation and interval arithmetic for estimation of disequilibrium models. Computational Economics, 10(3):295–316, 1997.<br/>\n",
    "David Sondak. Lectures 10 - 11. Harvard University. CS207 Fall 2019.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
